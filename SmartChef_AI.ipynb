{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPZekxV+K8520dqH/Qy23Ca",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FabianMareco/SMART-CHEF-AI/blob/main/SmartChef_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üç≥ SmartChef AI\n",
        "## Generador Inteligente de Recetas e Im√°genes con IA\n",
        "\n",
        "---\n",
        "\n",
        "## üìå Resumen\n",
        "\n",
        "SmartChef AI es una prueba de concepto (POC) que permite generar recetas personalizadas a partir de ingredientes disponibles ingresados por el usuario.\n",
        "\n",
        "Utilizando modelos de OpenAI:\n",
        "\n",
        "- Modelo texto-texto ‚Üí GPT-4o-mini\n",
        "- Modelo texto-imagen ‚Üí gpt-image-1\n",
        "\n",
        "El sistema crea tres recetas estructuradas y luego genera autom√°ticamente una imagen representativa de los platos propuestos.\n",
        "\n",
        "El proyecto demuestra el impacto del dise√±o estrat√©gico de prompts (Fast Prompting) en la calidad del output."
      ],
      "metadata": {
        "id": "JA0waaA4hNY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1Ô∏è‚É£ Introducci√≥n\n",
        "\n",
        "## Problem√°tica\n",
        "\n",
        "Muchas personas no saben qu√© cocinar con los ingredientes disponibles en su hogar, lo que genera desperdicio alimentario y decisiones improvisadas.\n",
        "\n",
        "## Propuesta\n",
        "\n",
        "Desarrollar una herramienta que:\n",
        "\n",
        "1. Reciba ingredientes como input.\n",
        "2. Genere 3 recetas estructuradas.\n",
        "3. Genere una imagen representativa autom√°ticamente."
      ],
      "metadata": {
        "id": "G619bm7_hQag"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EV8F3la3hQqg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q openai"
      ],
      "metadata": {
        "id": "BSJT5bUChTlX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from getpass import getpass\n",
        "import base64\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# üîê API segura\n",
        "api_key = getpass(\"üîê Ingresa tu API Key de OpenAI: \")\n",
        "\n",
        "client = OpenAI(api_key=api_key)\n",
        "\n",
        "print(\"‚úÖ API configurada correctamente\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Id6h6SNUhad5",
        "outputId": "414445bb-1e90-499c-b9c7-8d925821cd7b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîê Ingresa tu API Key de OpenAI: ¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑¬∑\n",
            "‚úÖ API configurada correctamente\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_recetas(ingredientes):\n",
        "\n",
        "    prompt = f\"\"\"\n",
        "    Act√∫a como un chef profesional experto en cocina pr√°ctica con ingredientes limitados.\n",
        "\n",
        "    TAREA:\n",
        "    Genera exactamente 3 recetas diferentes utilizando SOLO los siguientes ingredientes:\n",
        "\n",
        "    {ingredientes}\n",
        "\n",
        "    CONDICIONES:\n",
        "    - No agregar ingredientes externos.\n",
        "    - Si falta algo esencial, sugerir alternativa opcional.\n",
        "    - Cada receta debe incluir:\n",
        "        1. Nombre del plato\n",
        "        2. Descripci√≥n breve\n",
        "        3. Ingredientes utilizados\n",
        "        4. Paso a paso detallado\n",
        "        5. Tiempo estimado\n",
        "\n",
        "    FORMATO:\n",
        "    Respuesta clara en Markdown.\n",
        "    \"\"\"\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"Eres un chef profesional.\"},\n",
        "            {\"role\": \"user\", \"content\": prompt}\n",
        "        ],\n",
        "        temperature=0.7\n",
        "    )\n",
        "\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "tArdJ663hdFr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generar_imagen(prompt_imagen):\n",
        "\n",
        "    result = client.images.generate(\n",
        "        model=\"gpt-image-1\",\n",
        "        prompt=prompt_imagen,\n",
        "        size=\"1024x1024\"\n",
        "    )\n",
        "\n",
        "    image_base64 = result.data[0].b64_json\n",
        "    image_bytes = base64.b64decode(image_base64)\n",
        "\n",
        "    return image_bytes"
      ],
      "metadata": {
        "id": "2uwWS4H_hmdy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ingredientes_usuario = input(\"üçÖ Ingresa los ingredientes disponibles separados por coma: \")\n",
        "\n",
        "# Generar recetas\n",
        "recetas = generar_recetas(ingredientes_usuario)\n",
        "\n",
        "print(\"\\nüçΩ RECETAS GENERADAS:\\n\")\n",
        "print(recetas)\n",
        "\n",
        "# Crear prompt optimizado para imagen\n",
        "prompt_imagen = f\"\"\"\n",
        "Professional realistic food photography showing three different dishes:\n",
        "\n",
        "{recetas}\n",
        "\n",
        "Rustic wooden table.\n",
        "Warm lighting.\n",
        "High detail.\n",
        "4K resolution.\n",
        "\"\"\"\n",
        "\n",
        "# Generar imagen\n",
        "imagen_bytes = generar_imagen(prompt_imagen)\n",
        "\n",
        "# Mostrar imagen en Colab\n",
        "display(Image(imagen_bytes))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "VLsMRtqUhtVQ",
        "outputId": "6cd3164f-f957-4191-c865-039b23c5898e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üçÖ Ingresa los ingredientes disponibles separados por coma: arroz, pollo, huevos, oregano, cebolla, morron\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1329304045.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Generar recetas\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrecetas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerar_recetas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mingredientes_usuario\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nüçΩ RECETAS GENERADAS:\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-498494963.py\u001b[0m in \u001b[0;36mgenerar_recetas\u001b[0;34m(ingredientes)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \"\"\"\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     response = client.chat.completions.create(\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-4o-mini\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, prompt_cache_retention, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m   1190\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m   1191\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1192\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m   1193\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1194\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, content, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1295\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m         )\n\u001b[0;32m-> 1297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1299\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m                 \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1071\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'You exceeded your current quota, please check your plan and billing details. For more information on this error, read the docs: https://platform.openai.com/docs/guides/error-codes/api-errors.', 'type': 'insufficient_quota', 'param': None, 'code': 'insufficient_quota'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6Ô∏è‚É£ Resultados\n",
        "\n",
        "La implementaci√≥n permite:\n",
        "\n",
        "- Generar recetas coherentes con restricciones espec√≠ficas.\n",
        "- Reducir alucinaciones mediante role prompting.\n",
        "- Generar imagen autom√°ticamente v√≠a API.\n",
        "- Integrar modelo texto-texto y texto-imagen en una sola ejecuci√≥n.\n",
        "\n",
        "El sistema cumple completamente los objetivos planteados."
      ],
      "metadata": {
        "id": "QiLU6mdAh4e7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7Ô∏è‚É£ Conclusiones\n",
        "\n",
        "El proyecto demuestra que el dise√±o estrat√©gico de prompts:\n",
        "\n",
        "- Mejora la precisi√≥n del modelo.\n",
        "- Reduce respuestas inconsistentes.\n",
        "- Permite controlar estructura y formato.\n",
        "- Hace viable una soluci√≥n real con bajo costo computacional.\n",
        "\n",
        "La integraci√≥n de modelos multimodales valida la capacidad de OpenAI para resolver problemas cotidianos mediante IA generativa."
      ],
      "metadata": {
        "id": "boz3yOU8h5Fr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8Ô∏è‚É£ Referencias\n",
        "\n",
        "- OpenAI API Documentation\n",
        "- Material Diplomatura Full Stack Web Developer - Coderhouse"
      ],
      "metadata": {
        "id": "9BJSkAzPh8Sh"
      }
    }
  ]
}